<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="filter heuristics is a publication gathering heuristics-based cleaning approaches used to filter datasets for training LLM ">
    <meta name="keywords" content="AI, dataset, filtering, dirty data">
    <title>Filter Heuristics</title>
    <link rel="stylesheet" href="css/page.css">
</head>

<body class="project">
    <div id="page-wrap">
        <div class="text-post">
            <h2>Filter Heuristics</h2>
            <h3>a collection of dirty, naughty, obscene and otherwise bad holes</h3>
            <p>2024</p>
            <p class="reference">*more detailed documentation in progress* <br>
                +++ update: a new bonus chapter coming soon! instructions of how to make your data dirty and not to be
                trained!+++
            </p>
            <p>
                In 1980s, non-white women’s body size data was categorized as dirty data when establishing the first
                women's sizing system in US. Now in the age of GPT, what is considered as dirty data and how are they
                removed from massive training materials?
                <br>
                <br>
                Datasets nowadays for training large models have been expanded to the volume of (partial) internet*,
                with the idea of “scale averages out noise”, these datasets were scaled up by scrabbling whatever
                available data on the internet for free then “cleaned” with a human-not-in-the-loop,
                cheaper-than-cheap-labor method: heuristic filtering. Heuristics in this context are basically a set of
                rules came up by the engineers with their imagination and estimation that are “good enough” to remove
                “dirty data” of their perspective, not guaranteed to be optimal, perfect, or rational.
                <br>
                <br>
                The publication looks into 17 open (or reproduced thus open) extraction-based, human-not-in-the-loop
                datasets and gathers their heuristics for filtering out dirty data, with the question whether a
                narrative of “cleaning” in the context of high-tech will emerge from technical papers. It reflects on
                these silent, anonymous yet upheld estimations and not-guaranteed rationalities in current
                sociotechnical artifacts, and asks for whom these estimations are good-enough, as it will soon be part
                our technological infrastructures.
            </p>
                <br>
            <p class="reference">
                * some datasets appears as capturing the whole web, e.g. Common Crawl, but its one of its engineers
                <a target="_blank" rel="noopener" href="https://www.mozillafoundation.org/en/research/library/generative-ai-training-data/common-crawl/">clarified</a> that “Often it is claimed that Common Crawl contains the entire web, but that’s absolutely not
                true. Based on what I know about how many URLs exist, it’s very, very small.”
            </p>
            <p>
                <br>
                First edition published in April 2024.
            </p>

            <p>
                <br>

                Typefaces: Serifbabe SIGMA by Charlotte Rohde, Redaction by Jeremy Mickel, Neue Haas Grotesk.
            </p>


            <form action="index.html">
                <input type="submit" value="back">
            </form>
        </div>

        <div class="img-post-big">
            <img src="imgs/filter_heuristics/fh0.png" alt="">
            <img src="imgs/filter_heuristics/fh_book_0.jpg" alt="">
            <img src="imgs/filter_heuristics/fh_book_1.png" alt="">
            <img src="imgs/filter_heuristics/fh_book_2.png" alt="">
            <img src="imgs/filter_heuristics/fh_book_3.png" alt="">
            <img src="imgs/filter_heuristics/fh_book_4.jpg" alt="">
        </div>

        <div class="img-post-big">
            <img src="imgs/filter_heuristics/diagram-dataset.png" alt="">
            <img src="imgs/filter_heuristics/fh1.png" alt="">
            <img src="imgs/filter_heuristics/fh2.png" alt="">
            <img src="imgs/filter_heuristics/fh3.png" alt="">
            <img src="imgs/filter_heuristics/fh4.png" alt="">

            <form id="end-back" action="index.html">
                <input type="submit" value="back" />
            </form>
        </div>


    </div>

    <script src="js/jquery.js"></script>
    <script src="js/scroll.js"></script>


</body>

</html>